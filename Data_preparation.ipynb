{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_reactions\n",
    "from tokenization import smi_tokenizer\n",
    "from enumeration import get_alternative_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = get_reactions('../filtered.json', n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = get_reactions('../tokenized_reactions.json', n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reaction_core': '[H] - N ( - C ( - C ) = O ) - c ( : c ) : c ( : c ) - c ( : c ) : n ( - [H] ) : c >> c : c 1 : n : c ( - C ) : n ( : c ) : c ( : c ) : c : 1 : c',\n",
       " 'mapped_reaction': '[c:1]1(-[c:2]2[c:5]([N:10]([C:15]([C:18]([H:1018])([H:2018])[H:3018])=[O:19])[H:1010])[c:9]([H:1009])[c:14]([H:1014])[c:11]([H:1011])[c:6]2[H:1006])[n:3]([H:1003])[c:7]2[c:8]([c:4]1[H:1004])[c:13]([H:1013])[c:17]([H:1017])[c:16]([H:1016])[c:12]2[H:1012]>>[c:1]12[c:2]3[c:5]([c:9]([H:1009])[c:14]([H:1014])[c:11]([H:1011])[c:6]3[H:1006])[n:10][c:15]([C:18]([H:1018])([H:2018])[H:3018])[n:3]1[c:7]1[c:8]([c:4]2[H:1004])[c:13]([H:1013])[c:17]([H:1017])[c:16]([H:1016])[c:12]1[H:1012]',\n",
       " 'reaxys_id': 316610,\n",
       " 'meta': {'solvents': [],\n",
       "  'reagents': [{'reaxys_id': 1098214,\n",
       "    'molecule_name': 'hydrogenchloride',\n",
       "    'smiles': '[H]Cl'}],\n",
       "  'catalysts': []},\n",
       " 'reactants': ['[H] c 1 c ( [H] ) c ( [H] ) c ( N ( [H] ) C ( = O ) C ( [H] ) ( [H] ) [H] ) c ( - c 2 c ( [H] ) c 3 c ( [H] ) c ( [H] ) c ( [H] ) c ( [H] ) c 3 n 2 [H] ) c 1 [H]'],\n",
       " 'products': ['[H] c 1 c ( [H] ) c ( [H] ) c 2 c ( n c ( C ( [H] ) ( [H] ) [H] ) n 3 c 4 c ( [H] ) c ( [H] ) c ( [H] ) c ( [H] ) c 4 c ( [H] ) c 2 3 ) c 1 [H]']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = tokenized[0]['products'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_smiles = \"\".join(prod.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c1cccc2nc(n3c4c(cc3c21)cccc4)C']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Enumerator.get_alternative_smiles([prod_smiles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[H]c1c([H])c([H])c2c(nc(C([H])([H])[H])n3c4c([H])c([H])c([H])c([H])c4c([H])c23)c1[H]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-b14d5e084cce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "float(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "if x and x > 5:\n",
    "    print('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Chem.MolToSmiles(Chem.MolFromSmiles(prod_smiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C c 1 n c 2 c c c c c 2 c 2 c c 3 c c c c c 3 n 1 2'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smi_tokenizer(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[H] c 1 c ( [H] ) c ( [H] ) c 2 c ( n c ( C ( [H] ) ( [H] ) [H] ) n 3 c 4 c ( [H] ) c ( [H] ) c ( [H] ) c ( [H] ) c 4 c ( [H] ) c 2 3 ) c 1 [H]']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered[0][\"products\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "def detokenize_entry(entry: dict, fields: list=[\"reactants\", \"products\"]):\n",
    "    for field in fields:\n",
    "        new = []\n",
    "        for entity in entry[field]:\n",
    "            new.append(\"\".join(entity.split()))\n",
    "        entry[field] = new\n",
    "    return entry\n",
    "\n",
    "\n",
    "def process_json_entry(entry : dict, n_copies : int = 2):\n",
    "    entry = detokenize_entry(entry)\n",
    "    processed = dict()\n",
    "    processed[\"reaxys_id\"] = entry[\"reaxys_id\"]\n",
    "    for i, reactant in enumerate(entry[\"reactants\"]):\n",
    "        processed[f\"reactant_{i}\"] = get_alternative_smiles(reactant, n_copies)\n",
    "    for i, product in enumerate(entry[\"products\"]):\n",
    "        processed[f\"product_{i}\"] = get_alternative_smiles(product, n_copies)\n",
    "    return processed\n",
    "\n",
    "\n",
    "def process_file(in_fname: str, out_fname: str, processor):\n",
    "    with open(in_fname, \"r\") as istream, open(out_fname, \"w\") as ostream:\n",
    "        for entry in tqdm_notebook(istream):\n",
    "            entry = processor(json.loads(entry))\n",
    "            if entry:\n",
    "                json.dump(entry, ostream)\n",
    "                ostream.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d6a71600484682922986eda04d171b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_file(\"../filtered.json\", \"../enumerated.json\", process_json_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "enumerated = get_reactions('../enumerated.json', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_enum_entry(entry: dict):\n",
    "    exp = re.compile(r\"^(reactant|product)_\\d+$\")\n",
    "    for key in entry.keys():\n",
    "        if not exp.match(key):\n",
    "            continue\n",
    "        entry[key] = [smi_tokenizer(smiles) for smiles in entry[key]]\n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f543c7f314437c910f567e57dc1180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_file(\"../enumerated.json\", \"../tokenized.json\", tokenize_enum_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(in_fname: str):\n",
    "    from collections import Counter\n",
    "    exp = re.compile(r\"^(reactant|product)_\\d+$\")\n",
    "    vocab = Counter()\n",
    "    with open(in_fname, 'r') as istream:\n",
    "        for entry in tqdm_notebook(istream):\n",
    "            entry = json.loads(entry)\n",
    "            for key in entry.keys():\n",
    "                if not exp.match(key):\n",
    "                    continue\n",
    "                for entity in entry[key]:\n",
    "                    for ch in entity.split():\n",
    "                        vocab[ch] += 1 \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734bb39ce7454cc3a664c64b4e7cb0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = analyze_vocab(\"../tokenized.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', 17483646),\n",
       " ('C', 13275628),\n",
       " ('(', 9786943),\n",
       " (')', 9786943),\n",
       " ('O', 5393506),\n",
       " ('1', 5383914),\n",
       " ('=', 3109566),\n",
       " ('2', 2499586),\n",
       " ('N', 1963470),\n",
       " ('n', 1098948),\n",
       " ('3', 804604),\n",
       " ('[C@H]', 621797),\n",
       " ('[C@@H]', 607477),\n",
       " ('-', 549960),\n",
       " ('F', 534856),\n",
       " ('Cl', 324275),\n",
       " ('S', 268748),\n",
       " ('Br', 259998),\n",
       " ('4', 201240),\n",
       " ('[nH]', 166702),\n",
       " ('/', 145400),\n",
       " ('#', 134792),\n",
       " ('B', 119684),\n",
       " ('[N+]', 95918),\n",
       " ('s', 95002),\n",
       " ('o', 93784),\n",
       " ('[Si]', 88170),\n",
       " ('[O-]', 78482),\n",
       " ('[C@]', 57751),\n",
       " ('[C@@]', 57079),\n",
       " ('5', 47476),\n",
       " ('\\\\', 36464),\n",
       " ('I', 34254),\n",
       " ('P', 31636),\n",
       " ('[N-]', 17494),\n",
       " ('6', 10700),\n",
       " ('7', 2498),\n",
       " ('[PH]', 902),\n",
       " ('[SiH]', 640),\n",
       " ('8', 578),\n",
       " ('[IH2]', 446),\n",
       " ('9', 130),\n",
       " ('[SiH2]', 106),\n",
       " ('[PH2]', 24),\n",
       " ('%10', 18),\n",
       " ('[SH]', 14),\n",
       " ('[SiH3]', 14),\n",
       " ('[NH+]', 14)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vocab.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_set = set([k for k, v in vocab.items() if v >= 93000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#',\n",
       " '(',\n",
       " ')',\n",
       " '-',\n",
       " '/',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '=',\n",
       " 'B',\n",
       " 'Br',\n",
       " 'C',\n",
       " 'Cl',\n",
       " 'F',\n",
       " 'N',\n",
       " 'O',\n",
       " 'S',\n",
       " '[C@@H]',\n",
       " '[C@H]',\n",
       " '[N+]',\n",
       " '[nH]',\n",
       " 'c',\n",
       " 'n',\n",
       " 'o',\n",
       " 's'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVEL1 = set([\"C\", \"H\", \"O\", \"N\", \"c\", \"h\", \"o\", \"n\"])\n",
    "LEVEL2 = set([\"Br\", \"S\", \"Cl\", \"B\", \"F\", \"br\", \"s\", \"cl\", \"b\", \"f\"])\n",
    "LEVEL3 = set([\"Si\", \"I\", \"P\", \"si\", \"i\", \"p\"])\n",
    "LEVEL4 = set([\"Sn\", \"Se\", \"Mg\", \"sn\", \"se\", \"mg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entry_tokenset(entry: dict)->set:\n",
    "    exp = re.compile(r\"^(reactant|product)_\\d+$\")\n",
    "    result = set()\n",
    "    for key in entry.keys():\n",
    "        if not exp.match(key):\n",
    "            continue\n",
    "        for entity in entry[key]:\n",
    "            for token in entity.split():\n",
    "                result.add(token)\n",
    "    return result\n",
    "\n",
    "\n",
    "def tokenset_is_subset(superset: set, entry: dict):\n",
    "    tokenset = get_entry_tokenset(entry)\n",
    "    return entry if tokenset <= superset else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = get_reactions(\"../tokenized.json\", n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_entry_tokenset(tokenized[0]) <= filter_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df1d24d93224f5081be4d18f62c9bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_file(\"../tokenized.json\",\n",
    "             \"../filtered_by_tokenset.json\",\n",
    "             partial(tokenset_is_subset, filter_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomizing smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from utils import get_reactions, get_vocab_from_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from math import log2\n",
    "import re\n",
    "from copy import copy\n",
    "\n",
    "\n",
    "class TokenDistribution:\n",
    "\n",
    "\n",
    "    def __init__(self, tokens, freqs = None, n_tries = 20, log_freqs=False):\n",
    "\n",
    "        if freqs is None:\n",
    "            tokens, freqs = zip(*tokens.items())\n",
    "            if log_freqs:\n",
    "                freqs = [log2(freq) for freq in freqs]\n",
    "            total = sum(freqs)\n",
    "            freqs = [freq/total for freq in freqs]\n",
    "\n",
    "        self.tokens = tokens\n",
    "        self.freqs = freqs\n",
    "        self.n_tries = n_tries # Number of tries to get a character different from previous\n",
    "        # Deducing optimal value of tries from freqs seems possible\n",
    "        # It may be token dependent function, such as expected value of geometric distribution\n",
    "        # Another option is to make self.tokens a set and sample from set difference\n",
    "        # self.tokens \\ {old_token}\n",
    "\n",
    "\n",
    "    def sample(self, orig_token=None):\n",
    "        # Works only in Python 3.6+\n",
    "        new_token = random.choices(self.tokens, self.freqs)[0]\n",
    "        i = 0\n",
    "        while new_token == orig_token and i < self.n_tries:\n",
    "            new_token = random.choices(self.tokens, self.freqs)[0]\n",
    "            i += 1\n",
    "        return new_token\n",
    "\n",
    "\n",
    "    def sample_(self, orig_token=None):\n",
    "        # Using numpy instead of standard random\n",
    "        new_token = np.random.choice(self.tokens, self.freqs)[0]\n",
    "        i = 0\n",
    "        while new_token == orig_token and i < self.n_tries:\n",
    "            new_token = random.choices(self.tokens, self.freqs)[0]\n",
    "            i += 1\n",
    "        return new_token\n",
    "\n",
    "\n",
    "    def sample_single_insertion(self) -> str:\n",
    "        # Just a single token sampled unconditionally\n",
    "        return self.sample()\n",
    "\n",
    "\n",
    "    def sample_insertion(self, length, mode=0) -> [str]:\n",
    "        # with length parameter\n",
    "        if mode == 0:\n",
    "            tokens = []\n",
    "            prev_token = None\n",
    "            for _ in range(length):\n",
    "                curr = self.sample(prev_token)\n",
    "                prev_token = curr\n",
    "                tokens.append(curr)\n",
    "        else:\n",
    "            tokens = [self.sample(None) for _ in range(length)]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class SmilesRandomizer:\n",
    "\n",
    "\n",
    "    def __init__(self, distr, p_sub, p_in, p_del, n_tries=10):\n",
    "        self.distr = distr\n",
    "        self.p_sub = p_sub\n",
    "        self.p_in = p_in\n",
    "        self.p_del = p_del\n",
    "        self.n_tries = n_tries # Number of tries to get incorrect string\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def is_correct(seq: str):\n",
    "        return Chem.MolFromSmiles(seq) is not None\n",
    "\n",
    "\n",
    "    def mutate(self, seq : list) -> (list, int, int, int):\n",
    "        length = len(seq)\n",
    "        subs, ins, dels = 0, 0, 0\n",
    "        new_seq = []\n",
    "        i = 0\n",
    "        while i < length:\n",
    "            throw = random.random()\n",
    "            if throw <= self.p_sub:\n",
    "                # Substitution\n",
    "                success = False\n",
    "                for j in range(self.n_tries):\n",
    "                    char = self.distr.sample(seq[i])\n",
    "                    temp = \"\".join(new_seq + [char] + seq[i+1:])\n",
    "                    if not self.is_correct(temp):\n",
    "                        new_seq.append(char)\n",
    "                        success = True\n",
    "                        break\n",
    "                if success:\n",
    "                    subs += 1\n",
    "                    i += 1\n",
    "                    continue\n",
    "            if throw > self.p_sub and throw <= self.p_sub + self.p_in:\n",
    "                # Insertion\n",
    "                success = False\n",
    "                for j in range(self.n_tries):\n",
    "                    char = self.distr.sample(seq[i])\n",
    "                    temp = \"\".join(new_seq  + [char] + seq[i:])\n",
    "                    if not self.is_correct(temp):\n",
    "                        new_seq.append(char)\n",
    "                        new_seq.append(seq[i])\n",
    "                        success = True\n",
    "                        break\n",
    "                if success:\n",
    "                    ins += 1\n",
    "                    i += 1\n",
    "                    continue\n",
    "            if throw > self.p_sub + self.p_in and throw <= self.p_sub + self.p_in + self.p_del:\n",
    "                # Deletion\n",
    "                temp = \"\".join(new_seq + seq[i+1:])\n",
    "                if not self.is_correct(temp):\n",
    "                    dels += 1\n",
    "                    i += 1\n",
    "                    continue\n",
    "            new_seq.append(seq[i])\n",
    "            i += 1\n",
    "        return new_seq, subs, ins, dels\n",
    "        \n",
    "        \n",
    "def randomize_entry(randomizer: SmilesRandomizer, entry: dict):\n",
    "    exp = re.compile(r\"^(reactant|product)_\\d+$\")\n",
    "    for key in entry.keys():\n",
    "        if not exp.match(key):\n",
    "            continue\n",
    "        new_field = []\n",
    "        for entity in entry[key]:\n",
    "            print(entity)\n",
    "            new_entity, subs, ins, dels = randomizer.mutate(entity.split()) # Think about splits and joins\n",
    "            new_field.append(temp)\n",
    "        entry[field+'_MUTATED'] = new_field\n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc3c878804d4cb29b14a4fdf9430802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = get_vocab_from_tokenized(\"../filtered_by_tokenset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = TokenDistribution(vocab, log_freqs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomizer = SmilesRandomizer(td, 0.05, 0.05, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reacts = get_reactions('../filtered_by_tokenset.json', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstr = reacts[0]['product_0'][0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtstr = randomizer.mutate(tstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['c', '1', 'c', 'c', '2', 'c', '(', 'n', 'c', '(', 'n', '3', 'c', 'c', 'c', '3', 'c', '2', 'c', 'F', 'c', '2', ')', 'C', ')', 'c', '1'], 1, 1, 4)\n",
      "['c', '1', 'c', 'c', '2', 'c', '(', 'n', 'c', '(', 'n', '3', 'c', '2', 'c', 'c', '2', 'c', '3', 'c', 'c', 'c', 'c', '2', ')', 'C', ')', 'c', 'c', '1']\n"
     ]
    }
   ],
   "source": [
    "print(mtstr)\n",
    "print(tstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing[-4] = '('"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SmilesRandomizer.is_correct(\"\".join(mtstr[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "def make_tab_line(*args):\n",
    "    args = [str(x) if not isinstance(x, str) else x for x in args]\n",
    "    return \"\\t\".join(args) + \"\\n\"\n",
    "\n",
    "def make_randomized_sample(in_fname: str, out_fname: str, randomizer):\n",
    "    exp = re.compile(r\"^(reactant|product)_\\d+$\")\n",
    "    with open(in_fname, \"r\") as istream, open(out_fname, \"w\") as ostream:\n",
    "        ostream.write(make_tab_line(\"reaxys_id\", \"field_name\", \"original\", \"erroneous\", \"subs\", \"ins\", \"dels\"))\n",
    "        for entry in tqdm_notebook(istream):\n",
    "            entry = json.loads(entry)\n",
    "            for key in entry.keys():\n",
    "                if not exp.match(key):\n",
    "                    continue\n",
    "                for entity in entry[key]:\n",
    "                    new_ent, subs, ins, dels = randomizer.mutate(entity.split())\n",
    "                    line = make_tab_line(entry['reaxys_id'],\n",
    "                                         key,\n",
    "                                         entity,\n",
    "                                         \" \".join(new_ent),\n",
    "                                         subs,\n",
    "                                         ins,\n",
    "                                         dels)\n",
    "                    ostream.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163ffd5bc2604be2badc1b36d3927ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "make_randomized_sample(\"../filtered_by_tokenset.json\", \"train_sample.tsv\", randomizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_sample.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c 1 c 2 c ( c c ( - c 3 c ( N C ( C ) = O ) c c c c 3 ) [nH] 2 ) c c c 1'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"original\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
