{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, dataloader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "class DataSplitter:\n",
    "\n",
    "    def __init__(self, split_rate=0.8):\n",
    "        self.split_rate = split_rate\n",
    "        self.train_ids = None\n",
    "        self.test_ids = None\n",
    "\n",
    "    def split(self, infname: str, sep='\\t', id_field='reaxys_id'):\n",
    "        df = pd.read_csv(infname, sep=sep)\n",
    "        unique_ids = df[id_field].unique()\n",
    "        n_ids = unique_ids.shape[0]\n",
    "        self.train_ids = np.random.choice(unique_ids,\n",
    "                                          replace=False,\n",
    "                                          size=int(self.split_rate*n_ids))\n",
    "        self.test_ids = np.array([x for x in unique_ids if x not in self.train_ids])\n",
    "        train = df[df[id_field].isin(self.train_ids)]\n",
    "        test = df[df[id_field].isin(self.test_ids)]\n",
    "        return train, test\n",
    "        \n",
    "\n",
    "class SmilesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, infname: str, sep='\\t', fields_to_leave=['original', 'erroneous']):\n",
    "        df = pd.read_csv(infname, sep=sep)\n",
    "        df = df[fields_to_leave]\n",
    "        # Tokenize\n",
    "        self.token_map = self.get_token_map(df, fields_to_leave)\n",
    "        self.num_to_token = {v : k for k, v in self.token_map.items()}\n",
    "        df['erroneous'] = df['erroneous'].apply(lambda x: \"BOS \" + x + \" EOS\")\n",
    "        df['original'] = df['original'].apply(lambda x: \"BOS \" + x + \" EOS\")\n",
    "        self.inputs = [torch.LongTensor([self.token_map[t] for t in row.split()]) for row in df['erroneous']]\n",
    "        self.labels = [torch.LongTensor([self.token_map[t] for t in row.split()]) for row in df['original']]\n",
    "        self.input_lens = torch.LongTensor([len(x) for x in self.inputs])\n",
    "        self.label_lens = torch.LongTensor([len(x) for x in self.labels])\n",
    "        #max_inp_len = torch.max(self.input_lens).item()\n",
    "        #max_label_len = torch.max(self.label_lens).item()\n",
    "        #pad_len = max(max_inp_len, max_label_len)\n",
    "        #self.inputs = torch.LongTensor([x + [0] * (pad_len - len(x)) for x in self.inputs])\n",
    "        #self.labels = torch.LongTensor([x + [0] * (pad_len - len(x)) for x in self.labels])\n",
    "        #assert self.inputs.shape[0] == self.labels.shape[0]\n",
    "        #assert self.inputs.shape[1] == self.labels.shape[1]\n",
    "        #assert self.input_lens.shape[0] == self.label_lens.shape[0]\n",
    "        \n",
    "    def save_token_map(self, out_fname: str):\n",
    "        with open(out_fname, \"w\") as ostream:\n",
    "            ostream.write(json.dumps(self.token_map))\n",
    "        \n",
    "    def get_token_map(self, df, fields_to_leave):\n",
    "        token_map = {'PAD': 0, 'BOS': 1, 'EOS': 2}\n",
    "        start_idx = 3\n",
    "        token_set = set()\n",
    "        for field in fields_to_leave:\n",
    "            series = df[field]\n",
    "            for entry in series:\n",
    "                curr_set = set(entry.split())\n",
    "                token_set = token_set.union(curr_set)\n",
    "        for token in token_set:\n",
    "            token_map[token] = len(token_map)\n",
    "        return token_map\n",
    "    \n",
    "    @property\n",
    "    def n_tokens(self):\n",
    "        return len(self.token_map)\n",
    "\n",
    "    def as_smiles(self, tokenized: list):\n",
    "        return \"\".join([self.num_to_token[x] for x in tokenized])\n",
    "\n",
    "    def __len__(self):\n",
    "        #return self.inputs.shape[0]\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx], self.input_lens[idx], self.label_lens[idx]\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate(data):\n",
    "        inps, labs, in_lens, lab_lens = zip(*data)\n",
    "        inps = pad_sequence(inps)\n",
    "        labs = pad_sequence(labs)\n",
    "        in_lens = torch.stack(in_lens)\n",
    "        lab_lens = torch.stack(lab_lens)\n",
    "        return inps, labs, in_lens, lab_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SmilesDataset('train_split.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.62799e+05, 3.81716e+05, 2.76385e+05, 1.03958e+05, 3.08330e+04,\n",
       "        1.16140e+04, 5.01600e+03, 2.15700e+03, 6.15000e+02, 1.04000e+02]),\n",
       " array([  3. ,  22.4,  41.8,  61.2,  80.6, 100. , 119.4, 138.8, 158.2,\n",
       "        177.6, 197. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dataset.label_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=SmilesDataset.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([118, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(num_embeddings=dataset.n_tokens, embedding_dim=128, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = PositionalEncoding(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 1, 128])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe.pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pe(emb(batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([118, 64, 128])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(dataset.n_tokens, 64, 2, 64, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b27c26d56542>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    214\u001b[0m                                     \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                                     \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                                     memory_key_padding_mask=memory_key_padding_mask)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \"\"\"\n\u001b[1;32m    324\u001b[0m         tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n\u001b[0;32m--> 325\u001b[0;31m                               key_padding_mask=tgt_key_padding_mask)[0]\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    781\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   3098\u001b[0m     \u001b[0mkv_same\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3100\u001b[0;31m     \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3101\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0membed_dim_to_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3102\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "model.decoder(out[0][:1], out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_emb, d_model, n_heads, h_dim, n_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_emb, d_model, padding_idx=0)\n",
    "        self.positional = PositionalEncoding(d_model)\n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model, n_heads, h_dim),\n",
    "                                             n_layers)\n",
    "        self.decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(d_model, n_heads, h_dim),\n",
    "                                             n_layers)\n",
    "        self.out_linear = nn.Linear(d_model, n_emb)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_square_subsequent_mask(size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, x, max_iter=200, bos_idx=1):\n",
    "        # Work in progress\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional(x)\n",
    "        memory = self.encoder(x)\n",
    "        curr_seq = torch.zeros(max_iter, x.shape[0], dtype=torch.long)\n",
    "        curr_seq[0] = torch.full((x.shape[0], ), bos_idx)\n",
    "        logits = []\n",
    "        return curr_seq, memory\n",
    "        for i in range(max_iter):\n",
    "            x = self.decoder(curr_seq[:i+1], memory)\n",
    "            x = self.out_linear(x)\n",
    "            return x\n",
    "        return x\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, log_folder: str, weight_folder: str):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = model.to(self.device)\n",
    "        self.log_folder = log_folder\n",
    "        self.weight_folder = weight_folder\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        pass\n",
    "\n",
    "    def log(self):\n",
    "        pass\n",
    "    \n",
    "    def save_model(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.zeros(200, 21, dtype=\"torch.Long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 21])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full((1,), 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
